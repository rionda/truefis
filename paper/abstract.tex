\begin{abstract} 
   Frequent Itemsets (FIs) mining is a fundamental primitive in knowledge
   discovery. It
   requires to identify %that identifies 
   all itemsets appearing in at least a fraction %at least
   $\theta$ of a transactional dataset $\Ds$. Often though, the ultimate goal
   of mining $\Ds$ is not an analysis of the dataset \emph{per se}, but the
   understanding of the underlying process that generated it. %$\Ds$
   Specifically, in many applications $\Ds$ is a collection of samples obtained from an
   unknown probability distribution $\prob$ on transactions, and by extracting
   the FIs in $\Ds$ one attempts to infer itemsets that are
   frequently (i.e., with probability at least $\theta$) generated by $\prob$, which we call the True Frequent Itemsets
   (TFIs). Due to the inherently stochastic nature of the generative process, the
   set of FIs is only a rough approximation of the set of TFIs, as it %may contain
   often contains
   a huge number of \emph{false positives}, %spurious itemsets, 
   i.e., spurious itemsets that are not among the
   TFIs. In this work we design and analyze an algorithm %method 
   to identify a threshold $\hat{\theta}$ such that the collection of itemsets
   %of $\Ds$ 
   with frequency at least $\hat{\theta}$ in $\Ds$
   contains only TFIs with probability at least $1-\delta$, for
   some user-specified $\delta$. Our method uses %makes use of 
   results from statistical learning theory involving the (empirical) VC-dimension of the
   problem at hand. This allows us to identify %larger fraction of the TFIs
   almost all the TFIs without including any false positive. %(i.e., to achieve higher statistical power) 
   %than what could be done using traditional methods  %multiple hypothesis testing
   %corrections. 
   We also experimentally compare our method with the direct mining of $\Ds$ at
   frequency $\theta$ and with
   techniques based on widely-used standard bounds (i.e., the Chernoff bounds)
   of the binomial distribution, and show that our algorithm outperforms these methods and
   achieves even better results than what is guaranteed by the theoretical
   analysis.
   %s permits the identification of a very large fraction %subset of the TFIs, without reporting any false positive on real datasets.
   %while controlling the probability of reporting spurious itemsets.
     %This allows us to achieve, on the %same data, much stricter bounds
   %on the FWER than what could be done using
   %traditional multiple hypothesis testing corrections, or even using more
   %recent techniques as the hold-out. In our experimental evaluation we show
   %empirically that our test has very high statistical power, i.e., the output
   %collection contains a large fraction of the RFI's, while correctly bounding
   %the FWER.
 \end{abstract}

%{\bf Categories and Subject Descriptors:} H.2.8 [Database Management]: Database Applications -- \emph{Data Mining}

{\bf Keywords:} Frequent itemsets, VC-dimension, False positives,
Distribution-free methods, Frequency threshold identification, Pattern mining,
Significant patterns.

