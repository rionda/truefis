\section{Introduction}\label{sec:intro}

The extraction of association rules is one of the fundamental primitives in data
mining and knowledge discovery from large databases~\citep{AgrawalIS93}.  In its
most general definition, the problem can be reduced to identifying frequent sets
of items, or \emph{Frequent Itemsets} (FIs), appearing in at least a fraction
$\theta$ of all transactions in a dataset, where $\theta$ is provided in input
by the user. Frequent itemsets and association rules are not only of interest
for classic data mining applications (e.g., market basket analysis), but are
also useful for further data analysis and mining task, including clustering,
classification, and indexing~\citep{han2006data,HanCXY07}.

In most applications, the collection of FIs is not interesting \emph{per se}.
Instead, the mining results are used to infer properties of the \emph{underlying
process} that generated the dataset. Consider for example the following
scenario: a team of researchers would like to identify frequent associations
(i.e., itemsets) between preferences among Facebook users. To this end, they set
up an online survey which is filled out by a \emph{small fraction} of Facebook
users (some users may even take the survey multiple times). Using this
information, the researchers want to infer the associations (itemsets) that are
frequent for the \emph{entire} Facebook population. In fact, the whole Facebook
population and the online survey define the underlying \emph{process} that
generated the dataset \emph{observed} by the researchers. In this work we are
interested in answering the following question: how can we use the latter (the
observed dataset) to identify itemsets that are frequent in the former (the
whole population)? This is a very natural question, as is the underlying
assumption that the observed dataset is \emph{representative} of the generating
process. For example, in market basket analysis, the observed purchases of
customers are used to infer the future purchase habits of all customers while
assuming that the purchase behavior that generated the dataset is representative
of the one that will be followed in the future.

A natural and general model to describe these concepts is to assume that the
transactions in the dataset $\Ds$ are \emph{independent identically distributed}
(i.i.d.) samples from an \emph{unknown} probability distribution $\prob$ defined
on all possible transactions built on a set of items. Since $\prob$ is fixed,
each itemset $A$ has a fixed (unknown) \emph{probability} $\tfreq(A)$ to appear
in a transaction sampled from $\prob$. We call $\tfreq(A)$ the \emph{true
frequency} of $A$ (w.r.t.~$\prob$). The true frequency corresponds to the
fraction of transactions that would contain the itemset $A$ among an
hypothetical infinite set of transactions. The real goal of the mining process
is then to identify itemsets that have true frequency $\tfreq$ at least
$\theta$, i.e., the \emph{True Frequent Itemsets} (TFIs). In the market basket
analysis example, $\Ds$ contains the observed purchases of customers, the
\emph{unknown} distribution $\prob$ describes the purchase behavior of the
customers as a whole, and we want to analyze $\Ds$ to find the itemsets that
have probability (i.e., true frequency) at least $\theta$ to be bought by a
customer. Note that we made no assumption on $\prob$, except from the fact that
the transactions in the dataset are i.i.d.~samples from $\prob$. This is in
contrast to other settings that assume that the generative distribution $\prob$
is such that items appear in transactions generated by $\prob$ totally or
partially independently from each
other~\citep{SilversteinBM98,MegiddoS98,DuMouchelP01,GionisMMT07,Hamalainen10,KirschMAPUV12}.

Since $\Ds$ represents only a \emph{finite} sample from $\prob$, the set $F$ of frequent itemsets
of $\Ds$ w.r.t.~$\theta$ only provides an \emph{approximation} of the True Frequent Itemsets:
due to the stochastic nature of the generative process, the set $F$ may contain
a number of \emph{false positives}, i.e., itemsets that appear among the
frequent itemsets of $\Ds$ but whose \emph{true} frequency is smaller than
$\theta$.  At the same time, some itemsets with true frequency greater than
$\theta$ may have a frequency in $\Ds$ that is \emph{smaller} than $\theta$
(\emph{false negatives}), and therefore not be in $F$. This implies that one can
not aim at identifying \emph{all and only} the itemsets having true frequency at
least $\theta$. Even worse, from the data analyst's point of view, there is
\emph{no guarantee or bound on the number of false positives} reported in $F$.
Consider the following scenario as an example. Let $A$ and $B$ be two (disjoint)
sets of pairs of items. The set $A$ contains 1,000 disjoint pairs, while $B$
contains 10,000 disjoint pairs. Let $\prob$ be such that, for any pair
$(a,a')\in A$, we have $\tfreq((a,a'))=0.1$, and for any pair $(b,b')\in B$, we
have $\tfreq((b,b'))=0.09$. Let $\Ds$ be a dataset of 10,000 transactions
sampled from $\prob$. We are interested in finding pairs of items that have true
frequency at least $\theta=0.095$. If we extract the pairs of items with
frequency at least $\theta$ in $\Ds$, it is easy to see that in expectation 50
of the 1,000 pairs from $A$ will have frequency in $\Ds$ \emph{below} $0.095$,
and in expectation 400 pairs from $B$ will have frequency in $\Ds$ \emph{above}
$0.095$.  Therefore, the set of pairs that have frequency at least $\theta$ in
$\Ds$ does \emph{not} contain some of the pairs that have true frequency at
least $\theta$ (false negatives), but includes a huge number of pairs that have
true frequency smaller than $\theta$ (false positives).

In general, one would like to avoid false positives and at the same time find as
many TFIs as possible. These are somewhat contrasting goals, and care must be
taken to strike a good balance between them.  A na\"ive but \emph{overly
conservative} method to avoid false positives involves the use of \emph{Chernoff
and union bounds}~\citep{MitzenmacherU05}.  Let $A$ be an itemset in $\Ds$, and
$f_\Ds(A)$ be its frequency in the dataset, i.e., the fraction of transactions
of $\Ds$ that contain $A$. The quantity $|\Ds|f_\Ds(A)$ is a random variable
with Binomial distribution $\mathcal{B}(|\Ds|,\tfreq(A))$. It is possible to use
standard methods like the Chernoff and the union bounds to bound the deviation
of the frequencies in the dataset of \emph{all} itemsets from their
expectations. These tools can be used to compute a value $\hat\theta$ such that
the probability that a non-true frequent itemset $B$ has frequency greater or
equal to $\hat\theta$ is at most $1-\delta$, for some $\delta\in(0,1)$. This
method has the following serious drawback: in order to achieve such guarantee,
it is \emph{necessary} to bound the deviation of the frequencies of \emph{all
itemsets possibly appearing in the dataset}~\citep{KirschMAPUV12}. This means
that, if the transactions are built on a set of $n$ items, the union bound must
be taken over all $2^n-1$ potential itemsets, even if some or most of them may
appear with very low frequency or not at all in samples from $\prob$. As a
consequence, the chosen value of $\hat\theta$ is extremely \emph{conservative},
despite being sufficient to avoid the inclusion of false positives in the mining
results. The collection of itemsets with frequency at least $\hat\theta$ in
$\Ds$, although consisting (probabilistically) only of TFIs, only contains a
\emph{very small} portion of them, due to the overly conservative choice of
$\hat\theta$. (The results of our experimental evaluation in
Sect.~\ref{sec:experiments} clearly show the limitations of this method.) More
refined algorithms are therefore needed to achieve the correct balance between
the contrasting goals of avoiding false positives and finding as many TFIs as
possible.

\subsection{Our contributions.}
The contributions of this work are the following:
\begin{itemize*}
	\item We formally define the problem of mining the \emph{True Frequent
	Itemsets} w.r.t.~a minimum threshold $\theta$, and we develop and analyze an
	algorithm to \emph{identify a value $\hat{\theta}$ such that, with
	probability at least $1-\delta$, all itemsets with frequency at least
	$\hat{\theta}$ in the dataset have true frequency at least $\theta$}. Our
	method is completely \emph{distribution-free}, i.e., it does not make
	\emph{any} assumption about the unknown generative distribution $\prob$. By
	contrast, existing methods to assess the significance of frequent patterns
	after their extraction require a well specified, limited generative model to
	characterize the significance of a pattern. Our method also allows to
	include additional prior information about the distribution $\prob$, when
	available, to obtain even higher accuracy.
	\item We analyze our algorithm using results from \emph{statistical learning
	theory} and \emph{optimization}. We define a range set associated to a
	collection of itemsets and give an upper bound to its (empirical)
	VC-dimension and a procedure to compute this bound, showing an interesting
	connection with the Set-Union Knapsack Problem
	(SUKP)~\citep{GoldschmidtNY94}. To the best of our knowledge, ours is the
	first work to apply these techniques to the field of TFIs, and in general
	the first application of the sample complexity bound based on
	\emph{empirical} VC-dimension to the field of data mining.
	\item We implemented our algorithm and assessed its performances on
	simulated datasets with properties (number of items, itemsets frequency
	distribution, etc.) similar to real datasets. We computed the fraction of
	TFIs contained in the set of frequent itemsets in $\Ds$ w.r.t.~$\hat\theta$,
	and the number of false positives, if any. The results show that the
	algorithm is even \emph{more accurate} than the theory guarantees, since
	\emph{no false positive} is reported in any of the many experiments we
	performed, and moreover allows the \emph{extraction of almost all TFIs}. We
	also compared the set of itemsets computed by our method to those obtained
	with the ``Chernoff and union bounds'' method presented in the introduction,
	and found that our algorithm \emph{vastly outperforms} it.
\end{itemize*}


\paragraph*{Outline.}
In Sect.~\ref{sec:prevwork} we review relevant previous contributions.
Sections~\ref{sec:prelims} and~\ref{sec:range} contain preliminaries to formally define the problem
and key concepts that we will use throughout the work. Our proposed algorithm is
described and analyzed in Sect.~\ref{sec:main}. We present the methodology and
results of our experimental evaluation in Sect.~\ref{sec:experiments}.
Conclusions and future directions can be found in Sect.~\ref{sec:concl}.

%This is an example of a statistical test, in which the probability, or $p$-value,
%that a measure, or \emph{statistic} (in the example, the frequency of $\{a,b\}$)
%is at least as extreme as the value observed in real data is computed under a
%\emph{null hypothesis} that captures the properties of spurious discoveries (in
%the example, the independence of items). When the $p$-value is small enough, the
%itemset is flagged as significant, otherwise the itemset is discarded as a
%spurious discovery. A number of different
%procedures~\citep{SilversteinBM98,MegiddoS98,DuMouchelP01,GionisMMT07,Hamalainen10,KirschMAPUV12}
%have been proposed
%in recent years to control the number of spurious discoveries that are reported
%\emph{after} the frequent itemsets are identified. These procedures take into
%account the fact that a transactional dataset contains a number of patterns, and
%the assessment of their significance therefore is a \emph{multiple
%hypothesis testing} problem.
%
%
%For example, consider
%the transactions given by items that are bought together on Amazon; after
%observing a certain number of transactions, one is interested in inferring
%association between items that are valid for the distribution over \emph{all} possible
%purchases, not only for the current, observed set $\Ds$ of purchases that
%represents only a partial observation obtained from the distribution that
%includes also purchases that have not been recorded in the dataset, or that will
%materialize only in the future.
%
%The itemsets that are not frequent in $p$ but are frequent in $\Ds$ are false
%discoveries, and are not going to be filtered by the statistical tests described
%above, since such tests \emph{assume} that the itemsets that are frequent in
%$\Ds$, whose significance they assess, represent the frequent itemsets in $p$ as
%well. In fact, these tests define itemsets as spurious by considering properties
%of the itemsets other than its frequency. In the example above, the
%co-occurrence of $\{a,b\}$ is likely not due to random chance; however, the
%probability that $\{a,b\}$ appears with frequency $3\%$  in $\Ds$ while its
%frequency in $p$ is $1\%$ is $0.08$, therefore if $\theta=2\%$ then $\{a,b\}$
%likely is a false discovery. As noted by~\citet{LiuZW11}, the phase of
%assessment of the significance of the frequent itemsets cannot replace the role
%played by the minimum support threshold $\theta$, that is to reflect the level
%of domain significance, and is to be used in concert with statistical
%significance to filter uninteresting patterns that arise from different sources.
%Therefore being able to rigorously identify frequent patterns in $p$ is crucial
%in order to obtain high quality patterns.
%
%In this paper we address the problem of identifying itemsets that appear with
%probability at least $\theta$ in $p$, that we call \emph{True Frequent Itemsets}
%(RFI), while providing rigorous probabilistic guarantees on the number of false
%discoveries, without making any assumption on the particular generative model of
%the transactions. This makes our method completely \emph{distribution free}. In
%particular, we focus on returning a set of RFI with bounded Family-Wise Error
%Rate (FWER), that is the probability that one or more false discovery is
%reported among the RFI. A recently proposed alternative to  bound the FWER is to
%bound the False Discovery Rate (FDR), that in our case correspond to the
%proportion of false discoveries among the RFI. The use of the FDR allows to
%produce in output a larger number of patterns, since a small proportion of false
%discoveries are tolerated in the output; however, in data mining the number of
%patterns produced is usually high, therefore having a smaller number of high
%quality discoveries is preferable to reporting a larger number of patterns
%containing some false discoveries.
